{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a216e-930e-444a-986c-1a90ed436404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ProgressMeter, PyCall, PyPlot, Images, HDF5,NaNStatistics, Statistics, DSP, Lasso, JLD2\n",
    "using _Data, _Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff048e54-570b-4236-be33-f5a27cb5d6a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pyimport numpy\n",
    "@pyimport scipy.stats as stats\n",
    "@pyimport matplotlib.colors as mpl_colors\n",
    "@pyimport matplotlib.cm as cm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31453a66-b46a-4d17-8283-06067d83eabe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rc_params = PyDict(pyimport(\"matplotlib\")[\"rcParams\"]);\n",
    "rc_params[\"font.sans-serif\"] = [\"Arial\"];\n",
    "rc_params[\"font.size\"] = 7;\n",
    "rc_params[\"lines.linewidth\"] = 1;\n",
    "rc_params[\"lines.markersize\"] = 4;\n",
    "rc_params[\"xtick.major.size\"] = 2;\n",
    "rc_params[\"ytick.major.size\"] = 2;\n",
    "rc_params[\"xtick.major.pad\"] = 2;\n",
    "rc_params[\"ytick.major.pad\"] = 2;\n",
    "# rc_params[\"axes.labelpad\"] = 2;\n",
    "rc_params[\"axes.spines.right\"] = false\n",
    "rc_params[\"axes.spines.top\"] = false\n",
    "\n",
    "cim(img::Matrix{UInt32}) = CairoImageSurface(img, Cairo.FORMAT_RGB24; flipxy = false) \n",
    "cim(img::Matrix{ARGB32}) = cim(reinterpret(UInt32, img))\n",
    "cim(img::Matrix{RGB24}) = cim(reinterpret(UInt32, img))\n",
    "cim(img::Matrix{UInt8}) = cim(Gray.(reinterpret(N0f8, img)))\n",
    "cim(img::Array{UInt8,3}) = cim(RGB24.(reinterpret(N0f8, img[:,:,1]), reinterpret(N0f8, img[:,:,2]), reinterpret(N0f8, img[:,:,3])));downsample(img,n=4) = +([img[i:n:n*div(size(img,1)-1,n), j:n:n*div(size(img,2)-1,n)] for i = 1:n, j = 1:n]...)/(n*n);\n",
    "downsample(img,n=4) = +([img[i:n:n*div(size(img,1)-1,n), j:n:n*div(size(img,2)-1,n)] for i = 1:n, j = 1:n]...)/(n*n);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df0dbe-45d9-45d7-86ff-ea885476d004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# include(\"../../../functions/func_map.jl\")\n",
    "# include(\"../../../functions/func_stat.jl\")\n",
    "# include(\"../../../functions/func_data.jl\")\n",
    "include(\"../../../functions/func_plot.jl\")\n",
    "include(\"../../../functions/utils.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6be223-f24c-4a79-b38b-1b3d2a43e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path = \"/home/chuyu/Notebooks/project_place_cell/figures/output/sfigure3/\"\n",
    "mkpath(fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2c555-dcec-493d-98b7-35eb5b0a8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = \"chuyu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a0bbc-9bec-4310-82d8-8f0e4dc1e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_dict = load(\"/home/chuyu/Notebooks/project_place_cell/figures/chuyu/figure_data_info.jld2\")\n",
    "\n",
    "data_info_all = []\n",
    "for key in keys(data_info_dict)\n",
    "    append!(data_info_all, data_info_dict[key])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b7d514-a265-4fcf-9f2b-1d521949267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8a42d-3ba2-41bb-83cf-758ef4e7ad6e",
   "metadata": {},
   "source": [
    "# Percentage place cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d577abed-87b1-49d1-9f25-108bb865c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function neuron_region(region_roi_bool, region_name, neuron_label, n_neuron)\n",
    "    \n",
    "    # for one merged cell, it belongs to telecephalon if at least one of its roi belongs to telencephalon\n",
    "    region_roi_bool = region_roi_bool[:,findall(region_names .== region_name)][:,1]\n",
    "    whether_region = falses(n_neuron)\n",
    "    for which_neuron in Int32.(numpy.unique(neuron_label)[1:end-1])\n",
    "        if sum(region_roi_bool[neuron_label.==which_neuron]) >0\n",
    "            whether_region[which_neuron] = true\n",
    "        end\n",
    "    end\n",
    "    return whether_region\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1f773-84db-44a6-af90-018badd8ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_regions = [\"Telencephalon -\", \"Diencephalon -\", \"Mesencephalon -\", \"Rhombencephalon -\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72f74d-ad87-4325-9d0e-2a460f6f205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@showprogress for which_data = 1:length(data_info_all)\n",
    "    data_info = data_info_all[which_data]\n",
    "\n",
    "    experiment_filename_1 = data_info[1]\n",
    "    server_1 = data_info[2]\n",
    "\n",
    "    experiment_filename_2 = data_info[3]\n",
    "    server_2 = data_info[4]\n",
    "    experimenter = data_info[end]\n",
    "\n",
    "    ds_save_1 = Dataset(experiment_filename_1, experimenter, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_cy_1 = Dataset(experiment_filename_1, \"chuyu\", gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_analyzer_1 = Dataset(experiment_filename_1, analyzer, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    \n",
    "    ds_save_2 = Dataset(experiment_filename_2, experimenter, gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\") # This example dataset is on roli-9, so the path is different depending on whether you're trying to access the file from roli-9\n",
    "    ds_save_cy_2 = Dataset(experiment_filename_2, \"chuyu\", gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\")\n",
    "    ds_save_analyzer_2 = Dataset(experiment_filename_2, analyzer, gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\")\n",
    "\n",
    "\n",
    "    NMF_filename = joinpath(data_path(ds_save_cy_1), \"NMF_merge.h5\")\n",
    "    NMF_file = h5open(NMF_filename, \"r\")\n",
    "    global Z_all = HDF5.readmmap(NMF_file[\"Z_all\"])\n",
    "    global X_all = HDF5.readmmap(NMF_file[\"X_all\"])\n",
    "    global Y_all = HDF5.readmmap(NMF_file[\"Y_all\"])\n",
    "    global neuron_label = HDF5.readmmap(NMF_file[\"neuron_label\"])\n",
    "    close(NMF_file)\n",
    "\n",
    "    n_neuron = length(X_all);\n",
    "\n",
    "    # whether individual roi belongs to a certain region\n",
    "    region_bool_filename = joinpath(data_path(ds_save_cy_1), \"region_roi_bool.h5\")\n",
    "    region_bool_file = h5open(region_bool_filename, \"r\")\n",
    "    global region_names = read(region_bool_file, \"region_names\")\n",
    "    global region_roi_bool = read(region_bool_file, \"region_roi_bool\")\n",
    "    close(region_bool_file)\n",
    "\n",
    "\n",
    "\n",
    "    all_files = readdir(data_path(ds_save_cy_1))\n",
    "    long_name_files = all_files[findall([length(all_files[i])>6 for i in 1:length(all_files)])]\n",
    "    spatial_info_index = findall([long_name_files[i][1:6]==\"neuron\" for i in 1:length(long_name_files)])\n",
    "    candidate_filename = long_name_files[spatial_info_index]\n",
    "    which_file = [occursin(experiment_filename_1, candidate_filename[i])*occursin(\"A_dF\", candidate_filename[i]) for i in 1:length(candidate_filename)]\n",
    "    @assert(length(candidate_filename[which_file]) == 1)\n",
    "    save_file_name = candidate_filename[which_file][1]\n",
    "    n_bins = save_file_name[end-4:end-3]\n",
    "    info_filename = joinpath(data_path(ds_save_cy_1), save_file_name)\n",
    "    file = h5open(info_filename, \"r\")\n",
    "    place_map_all_1 = HDF5.readmmap(file[\"place_map_all\"])\n",
    "    specificity_1 = HDF5.readmmap(file[\"specificity\"])\n",
    "    specificity_population_z_1 = HDF5.readmmap(file[\"specificity_population_z\"])\n",
    "    specificity_shuffle_z_1 = HDF5.readmmap(file[\"specificity_shuffle_z\"])\n",
    "    valid_roi_1 = HDF5.readmmap(file[\"valid_neurons\"])\n",
    "    close(file)\n",
    "\n",
    "\n",
    "\n",
    "    place_cell_index_1 = intersect(findall(specificity_population_z_1.>3), findall(specificity_shuffle_z_1.>5), findall(specificity_1.>0.01));\n",
    "\n",
    "\n",
    "\n",
    "    whether_region_all = []\n",
    "    for region_name in which_regions\n",
    "        whether_region = neuron_region(region_roi_bool, region_name, neuron_label, n_neuron)\n",
    "        append!(whether_region_all, [whether_region])\n",
    "    end\n",
    "\n",
    "    place_cell_percentage_all = []\n",
    "    for whether_region in whether_region_all\n",
    "        region_neurons = findall(whether_region)\n",
    "        place_cell_percentage = sum(whether_in(region_neurons, place_cell_index_1))./length(region_neurons)\n",
    "        append!(place_cell_percentage_all, place_cell_percentage)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    region_place_cell_nr = [length(intersect(place_cell_index_1, findall(whether_region_all[i]))) for i in 1:length(whether_region_all)]\n",
    "    fraction_place_cell = region_place_cell_nr./length(place_cell_index_1)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    h5open(joinpath(data_path(ds_save_analyzer_1), \"place_cell_percentage.h5\"), \"w\") do file\n",
    "        file[\"place_cell_percentage_all\"] = Float32.(place_cell_percentage_all)\n",
    "        file[\"fraction_place_cell\"] = Float32.(fraction_place_cell)\n",
    "        file[\"which_regions\"] = which_regions\n",
    "\n",
    "    end;\n",
    "end\n",
    "\n",
    "@showprogress for which_data = 1:length(data_info_all)\n",
    "    data_info = data_info_all[which_data]\n",
    "\n",
    "    experiment_filename_1 = data_info[1]\n",
    "    server_1 = data_info[2]\n",
    "\n",
    "    experiment_filename_2 = data_info[3]\n",
    "    server_2 = data_info[4]\n",
    "    experimenter = data_info[end]\n",
    "\n",
    "    ds_save_1 = Dataset(experiment_filename_1, experimenter, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_cy_1 = Dataset(experiment_filename_1, \"chuyu\", gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_analyzer_1 = Dataset(experiment_filename_1, analyzer, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    \n",
    "    ds_save_2 = Dataset(experiment_filename_2, experimenter, gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\") # This example dataset is on roli-9, so the path is different depending on whether you're trying to access the file from roli-9\n",
    "    ds_save_cy_2 = Dataset(experiment_filename_2, \"chuyu\", gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\")\n",
    "    ds_save_analyzer_2 = Dataset(experiment_filename_2, analyzer, gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\")\n",
    "\n",
    "\n",
    "    NMF_filename = joinpath(data_path(ds_save_cy_1), \"NMF_merge.h5\")\n",
    "    NMF_file = h5open(NMF_filename, \"r\")\n",
    "    global Z_all = HDF5.readmmap(NMF_file[\"Z_all\"])\n",
    "    global X_all = HDF5.readmmap(NMF_file[\"X_all\"])\n",
    "    global Y_all = HDF5.readmmap(NMF_file[\"Y_all\"])\n",
    "    global neuron_label = HDF5.readmmap(NMF_file[\"neuron_label\"])\n",
    "    close(NMF_file)\n",
    "\n",
    "    n_neuron = length(X_all);\n",
    "\n",
    "    # whether individual roi belongs to a certain region\n",
    "    region_bool_filename = joinpath(data_path(ds_save_cy_1), \"region_roi_bool.h5\")\n",
    "    region_bool_file = h5open(region_bool_filename, \"r\")\n",
    "    global region_names = read(region_bool_file, \"region_names\")\n",
    "    global region_roi_bool = read(region_bool_file, \"region_roi_bool\")\n",
    "    close(region_bool_file)\n",
    "\n",
    "\n",
    "\n",
    "    all_files = readdir(data_path(ds_save_cy_2))\n",
    "    long_name_files = all_files[findall([length(all_files[i])>6 for i in 1:length(all_files)])]\n",
    "    spatial_info_index = findall([long_name_files[i][1:6]==\"neuron\" for i in 1:length(long_name_files)])\n",
    "    candidate_filename = long_name_files[spatial_info_index]\n",
    "    which_file = [occursin(experiment_filename_2, candidate_filename[i])*occursin(\"A_dF\", candidate_filename[i]) for i in 1:length(candidate_filename)]\n",
    "    @assert(length(candidate_filename[which_file]) == 1)\n",
    "    save_file_name = candidate_filename[which_file][1]\n",
    "    n_bins = save_file_name[end-4:end-3]\n",
    "    info_filename = joinpath(data_path(ds_save_cy_2), save_file_name)\n",
    "    file = h5open(info_filename, \"r\")\n",
    "    place_map_all_1 = HDF5.readmmap(file[\"place_map_all\"])\n",
    "    specificity_1 = HDF5.readmmap(file[\"specificity\"])\n",
    "    specificity_population_z_1 = HDF5.readmmap(file[\"specificity_population_z\"])\n",
    "    specificity_shuffle_z_1 = HDF5.readmmap(file[\"specificity_shuffle_z\"])\n",
    "    valid_roi_1 = HDF5.readmmap(file[\"valid_neurons\"])\n",
    "    close(file)\n",
    "\n",
    "\n",
    "\n",
    "    place_cell_index_1 = intersect(findall(specificity_population_z_1.>3), findall(specificity_shuffle_z_1.>5), findall(specificity_1.>0.01));\n",
    "\n",
    "\n",
    "\n",
    "    whether_region_all = []\n",
    "    for region_name in which_regions\n",
    "        whether_region = neuron_region(region_roi_bool, region_name, neuron_label, n_neuron)\n",
    "        append!(whether_region_all, [whether_region])\n",
    "    end\n",
    "\n",
    "    place_cell_percentage_all = []\n",
    "    for whether_region in whether_region_all\n",
    "        region_neurons = findall(whether_region)\n",
    "        place_cell_percentage = sum(whether_in(region_neurons, place_cell_index_1))./length(region_neurons)\n",
    "        append!(place_cell_percentage_all, place_cell_percentage)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    region_place_cell_nr = [length(intersect(place_cell_index_1, findall(whether_region_all[i]))) for i in 1:length(whether_region_all)]\n",
    "    fraction_place_cell = region_place_cell_nr./length(place_cell_index_1)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    h5open(joinpath(data_path(ds_save_analyzer_2), \"place_cell_percentage.h5\"), \"w\") do file\n",
    "        file[\"place_cell_percentage_all\"] = Float32.(place_cell_percentage_all)\n",
    "        file[\"fraction_place_cell\"] = Float32.(fraction_place_cell)\n",
    "        file[\"which_regions\"] = which_regions\n",
    "\n",
    "    end;\n",
    "end\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28042fd-be92-464a-9629-e0c0e11447b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun data\n",
    "data_info_all = \n",
    "[\n",
    "\n",
    "    [\"20220406_153842\", 9, \"jen\"],\n",
    "    [\"20220405_171444\", 25, \"jen\"],\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da246bb-a602-4ec3-a3b3-2b4108675c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for single session exp\n",
    "@showprogress for which_data = 1:length(data_info_all)\n",
    "    data_info = data_info_all[which_data]\n",
    "\n",
    "    experiment_filename_1 = data_info[1]\n",
    "    server_1 = data_info[2]\n",
    "\n",
    "    experimenter = data_info[end]\n",
    "\n",
    "    ds_save_1 = Dataset(experiment_filename_1, experimenter, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_cy_1 = Dataset(experiment_filename_1, \"chuyu\", gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_analyzer_1 = Dataset(experiment_filename_1, analyzer, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    \n",
    "\n",
    "\n",
    "    NMF_filename = joinpath(data_path(ds_save_cy_1), \"NMF_merge.h5\")\n",
    "    NMF_file = h5open(NMF_filename, \"r\")\n",
    "    global Z_all = HDF5.readmmap(NMF_file[\"Z_all\"])\n",
    "    global X_all = HDF5.readmmap(NMF_file[\"X_all\"])\n",
    "    global Y_all = HDF5.readmmap(NMF_file[\"Y_all\"])\n",
    "    global neuron_label = HDF5.readmmap(NMF_file[\"neuron_label\"])\n",
    "    close(NMF_file)\n",
    "\n",
    "    n_neuron = length(X_all);\n",
    "\n",
    "    # whether individual roi belongs to a certain region\n",
    "    region_bool_filename = joinpath(data_path(ds_save_cy_1), \"region_roi_bool.h5\")\n",
    "    region_bool_file = h5open(region_bool_filename, \"r\")\n",
    "    global region_names = read(region_bool_file, \"region_names\")\n",
    "    global region_roi_bool = read(region_bool_file, \"region_roi_bool\")\n",
    "    close(region_bool_file)\n",
    "\n",
    "\n",
    "\n",
    "    all_files = readdir(data_path(ds_save_cy_1))\n",
    "    long_name_files = all_files[findall([length(all_files[i])>6 for i in 1:length(all_files)])]\n",
    "    spatial_info_index = findall([long_name_files[i][1:6]==\"neuron\" for i in 1:length(long_name_files)])\n",
    "    candidate_filename = long_name_files[spatial_info_index]\n",
    "    which_file = [occursin(experiment_filename_1, candidate_filename[i])*occursin(\"A_dF\", candidate_filename[i]) for i in 1:length(candidate_filename)]\n",
    "    @assert(length(candidate_filename[which_file]) == 1)\n",
    "    save_file_name = candidate_filename[which_file][1]\n",
    "    n_bins = save_file_name[end-4:end-3]\n",
    "    info_filename = joinpath(data_path(ds_save_cy_1), save_file_name)\n",
    "    file = h5open(info_filename, \"r\")\n",
    "    place_map_all_1 = HDF5.readmmap(file[\"place_map_all\"])\n",
    "    specificity_1 = HDF5.readmmap(file[\"specificity\"])\n",
    "    specificity_population_z_1 = HDF5.readmmap(file[\"specificity_population_z\"])\n",
    "    specificity_shuffle_z_1 = HDF5.readmmap(file[\"specificity_shuffle_z\"])\n",
    "    valid_roi_1 = HDF5.readmmap(file[\"valid_neurons\"])\n",
    "    close(file)\n",
    "\n",
    "\n",
    "\n",
    "    place_cell_index_1 = intersect(findall(specificity_population_z_1.>3), findall(specificity_shuffle_z_1.>5), findall(specificity_1.>0.01));\n",
    "\n",
    "\n",
    "\n",
    "    whether_region_all = []\n",
    "    for region_name in which_regions\n",
    "        whether_region = neuron_region(region_roi_bool, region_name, neuron_label, n_neuron)\n",
    "        append!(whether_region_all, [whether_region])\n",
    "    end\n",
    "\n",
    "    place_cell_percentage_all = []\n",
    "    for whether_region in whether_region_all\n",
    "        region_neurons = findall(whether_region)\n",
    "        place_cell_percentage = sum(whether_in(region_neurons, place_cell_index_1))./length(region_neurons)\n",
    "        append!(place_cell_percentage_all, place_cell_percentage)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    region_place_cell_nr = [length(intersect(place_cell_index_1, findall(whether_region_all[i]))) for i in 1:length(whether_region_all)]\n",
    "    fraction_place_cell = region_place_cell_nr./length(place_cell_index_1)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    h5open(joinpath(data_path(ds_save_analyzer_1), \"place_cell_percentage.h5\"), \"w\") do file\n",
    "        file[\"place_cell_percentage_all\"] = Float32.(place_cell_percentage_all)\n",
    "        file[\"fraction_place_cell\"] = Float32.(fraction_place_cell)\n",
    "        file[\"which_regions\"] = which_regions\n",
    "\n",
    "    end;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0f0ea-49b6-4ae8-978e-6e2d5409cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pyimport pandas as pd\n",
    "@pyimport seaborn as sns\n",
    "\n",
    "@pyimport matplotlib.colors as mpl_colors\n",
    "@pyimport matplotlib.cm as cm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c40226-3c38-4d4e-affb-6cd5eb23996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot = cm.get_cmap(\"hot\", 100)\n",
    "alpha_top = 50\n",
    "cmaplist = [hot(i) for i in 1:(hot.N)]\n",
    "cmaplist_new = []\n",
    "length_cmaplist = length(cmaplist)\n",
    "for (i, color_) in enumerate(cmaplist)\n",
    "    new_color_ = numpy.copy(color_)\n",
    "    if i<=alpha_top\n",
    "        new_color_[4] = (100/alpha_top)*i/length_cmaplist\n",
    "    end\n",
    "    append!(cmaplist_new,[new_color_]) \n",
    "end\n",
    "hot_new = mpl_colors.LinearSegmentedColormap.from_list(\"hot_new\",cmaplist_new,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026cbab4-b64d-4ddc-a5be-7efc3d09d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_hedgehog_rotation_bottom_out = \n",
    "\n",
    "[\n",
    "    [\"20230406_100925\", 9, \"20230406_120147\", 9, \"chuyu\"],\n",
    "    [\"20230429_140408\", 8, \"20230429_153439\", 8, \"lorenz\"],\n",
    "]\n",
    "\n",
    "job_hedgehog_rotation_bottom_out_clean = \n",
    "\n",
    "[\n",
    "    [\"20230407_171016\", 9, \"20230407_184007\", 9, \"chuyu\"],\n",
    "    [\"20230429_194403\", 8, \"20230429_210211\", 8, \"jen\"],\n",
    "]\n",
    "\n",
    "\n",
    "job_hedgehog_landmark_removal = \n",
    "\n",
    "[\n",
    "    [\"20230409_141947\", 9, \"20230409_162256\", 9, \"drew\"],\n",
    "    [\"20230423_103330\", 8, \"20230423_122640\", 8,  \"jen\"],\n",
    "    [\"20230423_141713\", 8, \"20230423_155036\", 8,  \"drew\"],\n",
    "    [\"20230423_170228\", 8, \"20230423_182416\", 8,  \"drew\"],\n",
    "]\n",
    "\n",
    "job_hedgehog_circle = \n",
    "\n",
    "[\n",
    "\n",
    "    [\"20230422_160839\", 8, \"20230422_174803\", 8, \"lorenz\"],\n",
    "    [\"20230422_130558\", 8, \"20230422_143553\", 8, \"jen\"],\n",
    "    [\"20230617_164214\", 8, \"20230617_183124\", 8, \"lorenz\"],\n",
    "]\n",
    "\n",
    "job_landmark_removal = \n",
    "\n",
    "[\n",
    "\n",
    "    [\"20230419_143319\", 9, \"20230419_161007\", 9, \"chuyu\"],\n",
    "    [\"20230420_113755\", 9, \"20230420_130657\", 9, \"lorenz\"],\n",
    "    [\"20230420_152855\", 9, \"20230420_165502\", 9, \"chuyu\"],\n",
    "]\n",
    "\n",
    "job_boundary_morphing = \n",
    "\n",
    "[\n",
    "    [\"20230224_162111\", 2, \"20230224_171731\", 2, \"chuyu\"],\n",
    "    [\"20230223_135721\", 9, \"20230223_150240\", 9, \"chuyu\"],\n",
    "    [\"20230618_122231\", 8, \"20230618_134025\", 8, \"chuyu\"],\n",
    "    [\"20230618_155837\", 8, \"20230618_170437\", 8, \"chuyu\"],\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "datasets_corner_cue = \n",
    "[\n",
    "    [\"20220407_152537\", 4, \"jen\"],\n",
    "    [\"20220406_111526\", 9, \"jen\"],\n",
    "    [\"20220407_090156\", 5, \"jen\"],\n",
    "    [\"20220417_165530\", 2, \"jen\"],\n",
    "    [\"20220406_153842\", 9, \"jen\"],\n",
    "    [\"20220405_171444\", 25, \"jen\"],\n",
    "    [\"20220416_160516\", 6, \"jen\"]\n",
    "];\n",
    "\n",
    "\n",
    "\n",
    "job_corner_cue_change = \n",
    "\n",
    "[\n",
    "    [\"20220407_152537\", 4, \"20220407_170908\", 4, \"jen\"],\n",
    "    [\"20220818_123314\", 7, \"20220818_145358\", 7, \"chuyu\"],\n",
    "    [\"20220818_163902\", 7, \"20220818_182702\", 7, \"chuyu\"],\n",
    "    [\"20220819_094333\", 1, \"20220819_114922\", 1, \"chuyu\"],\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00038b6-75f8-4197-a3bf-f336b30f342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_hedgehog =  reduce(vcat, [job_hedgehog_rotation_bottom_out, job_hedgehog_rotation_bottom_out_clean, job_hedgehog_landmark_removal, job_hedgehog_circle])\n",
    "data_info_corner_cue =  reduce(vcat, [datasets_corner_cue])\n",
    "data_info_octagon =  reduce(vcat, [job_boundary_morphing])\n",
    "data_info_square_landmark =  reduce(vcat, [job_landmark_removal])\n",
    "\n",
    "data_info_circle =  reduce(vcat, [job_corner_cue_change])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076d8c7-282c-4188-8e58-612296e7d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Rectangular\",\"Asymmetric\", \"Square\",  \"Octagon\", \"Circle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b61e3-f99c-4662-ada9-c40ef8b3cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i_data, data_info_all) in enumerate([data_info_corner_cue, data_info_hedgehog,data_info_square_landmark, data_info_octagon, data_info_circle])\n",
    "    println(length(data_info_all))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf1c16-78a7-40bc-858a-177b93df18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_all = subplots(1,5,figsize=(5,2))\n",
    "for (i_data, data_info_all) in enumerate([data_info_corner_cue, data_info_hedgehog,data_info_square_landmark, data_info_octagon, data_info_circle])\n",
    "    ax = ax_all[i_data]\n",
    "    place_cell_percentage_all_fish = []\n",
    "    fraction_tel_place_cell_all_fish = []\n",
    "for which_data = 1:length(data_info_all)\n",
    "    data_info = data_info_all[which_data]\n",
    "\n",
    "    experiment_filename_1 = data_info[1]\n",
    "    server_1 = data_info[2]\n",
    "    experimenter = data_info[end]\n",
    "        \n",
    "    if labels[i_data] == \"Circle\"\n",
    "        experiment_filename_1 = data_info[3]\n",
    "        server_1 = data_info[4]\n",
    "        experimenter = data_info[end]\n",
    "    end\n",
    "\n",
    "    ds_save_1 = Dataset(experiment_filename_1, experimenter, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_cy_1 = Dataset(experiment_filename_1, \"chuyu\", gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_analyzer_1 = Dataset(experiment_filename_1, analyzer, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    \n",
    "\n",
    "\n",
    "    # whether individual roi belongs to a certain region\n",
    "    cell_percentage_filename = joinpath(data_path(ds_save_analyzer_1), \"place_cell_percentage.h5\")\n",
    "    cell_percentage_file = h5open(cell_percentage_filename, \"r\")\n",
    "    place_cell_percentage_all = read(cell_percentage_file, \"place_cell_percentage_all\")\n",
    "    fraction_place_cell = read(cell_percentage_file, \"fraction_place_cell\")\n",
    "    which_regions = read(cell_percentage_file, \"which_regions\")\n",
    "    close(cell_percentage_file)\n",
    "        \n",
    "    append!(place_cell_percentage_all_fish, [place_cell_percentage_all])\n",
    "    append!(fraction_tel_place_cell_all_fish, [fraction_place_cell])\n",
    "\n",
    "end\n",
    "    \n",
    "    \n",
    "interested_large_regions = [\"tel\", \"di\", \"mes\", \"rhomb\"]\n",
    "region_percentage_all = []\n",
    "for i in 1:length(place_cell_percentage_all_fish)\n",
    "    \n",
    "    region_percentage = Dict()\n",
    "    for (i_region, region_name) in enumerate(interested_large_regions)\n",
    "        region_percentage[region_name] = place_cell_percentage_all_fish[i][i_region]\n",
    "    end\n",
    "    \n",
    "    append!(region_percentage_all,[region_percentage])\n",
    "end\n",
    "\n",
    "region_fraction_all = []\n",
    "for i in 1:length(fraction_tel_place_cell_all_fish)\n",
    "    region_fraction = Dict()\n",
    "    for (i_region, region_name) in enumerate(interested_large_regions)\n",
    "        region_fraction[region_name] = fraction_tel_place_cell_all_fish[i][i_region]\n",
    "    end\n",
    "    \n",
    "    append!(region_fraction_all,[region_fraction])\n",
    "end\n",
    "    \n",
    "df = pd.DataFrame(region_percentage_all)\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "sns.stripplot(order = interested_large_regions, data=df,ax=ax, color=hot_new(60), s=3, alpha=0.8)\n",
    "if i_data == 1\n",
    "    ax.set_ylabel(\"# PCs per region / # cells per region\")\n",
    "end\n",
    "ax.spines[\"top\"].set_visible(false)\n",
    "ax.spines[\"right\"].set_visible(false)\n",
    "\n",
    "\n",
    "\n",
    "order = interested_large_regions\n",
    "for i in 0:length(order)-1\n",
    "    m = numpy.nanmedian([x[order[i+1]] for x in region_percentage_all])\n",
    "    ax.plot([i-0.3, i+0.3], [m, m], c=\"k\", lw=2)\n",
    "end\n",
    "ax.set_xticks([0,1,2,3])\n",
    "ax.set_xticklabels(labels= [\"Tel.\", \"Di.\", \"Mes.\", \"Rhomb.\"], rotation=45, ha=\"right\" , rotation_mode=\"anchor\")\n",
    "# yticks([0,500,1000,1500,2000],[0,\"\",\"\",\"\",2000])\n",
    "\n",
    "    \n",
    "ax.set_title(labels[i_data], fontsize=7)\n",
    "    \n",
    "ax.set_ylim(-0.01,0.25)\n",
    "    \n",
    "end\n",
    "tight_layout()\n",
    "\n",
    "\n",
    "fig.savefig(joinpath(fig_path, \"geometry_region_percentage.pdf\"), bbox_inches = \"tight\",transparent = true,pad_inches = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eca5b5-8ffe-4773-81b8-a23e4e2942d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
