{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb80ad-a795-41a0-82fc-8344a095ba0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ProgressMeter, PyCall, PyPlot, Images, HDF5,NaNStatistics, Statistics, DSP, Lasso, JLD2\n",
    "using _Data, _Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d266003-e053-42e1-85af-c2da4fd5c9ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pyimport numpy\n",
    "@pyimport sklearn.decomposition as decomposition\n",
    "@pyimport scipy.stats as stats\n",
    "@pyimport matplotlib.colors as mpl_colors\n",
    "@pyimport matplotlib.cm as cm \n",
    "@pyimport sklearn.cluster as cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa8bf0-3dc3-4d16-b18d-8658763d03ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "include(\"../../../functions/func_map.jl\")\n",
    "include(\"../../../functions/func_stat.jl\")\n",
    "include(\"../../../functions/func_data.jl\")\n",
    "include(\"../../../functions/func_plot.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb7863-f735-47da-9fb7-f4047a86c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_dict = load(\"/home/chuyu/Notebooks/project_place_cell/figures/chuyu/figure_data_info.jld2\")\n",
    "\n",
    "data_info_all = []\n",
    "for key in keys(data_info_dict)\n",
    "    if key == \"corner_cue_circle\"\n",
    "    append!(data_info_all, data_info_dict[key])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75537a70-3b3c-40d0-ab33-6a979c36489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20840dfd-571c-4e72-9944-3f08f8a00065",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = \"chuyu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8618dc-ff62-4a3c-a72c-da36bbf70b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_data = 1\n",
    "    data_info = data_info_all[which_data]\n",
    "\n",
    "    experiment_filename_1 = data_info[1]\n",
    "    server_1 = data_info[2]\n",
    "\n",
    "    experiment_filename_2 = data_info[3]\n",
    "    server_2 = data_info[4]\n",
    "\n",
    "    experimenter = data_info[5]\n",
    "\n",
    "    ds_save_1 = Dataset(experiment_filename_1, experimenter, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_cy_1 = Dataset(experiment_filename_1, \"chuyu\", gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_analyzer_1 = Dataset(experiment_filename_1, analyzer, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "\n",
    "    ds_save_2 = Dataset(experiment_filename_2, experimenter, gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\") # This example dataset is on roli-9, so the path is different depending on whether you're trying to access the file from roli-9\n",
    "    ds_save_cy_2 = Dataset(experiment_filename_2, \"chuyu\", gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\")\n",
    "    ds_save_analyzer_2 = Dataset(experiment_filename_2, analyzer, gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\")\n",
    "\n",
    "\n",
    "    file_folder_1 = joinpath(data_path(ds_save_cy_1), \"\")\n",
    "    all_files = readdir(file_folder_1)\n",
    "    long_name_files = all_files[findall([length(all_files[i])>6 for i in 1:length(all_files)])]\n",
    "    spatial_info_index = findall([long_name_files[i][1:6]==\"neuron\" for i in 1:length(long_name_files)])\n",
    "    candidate_filename = long_name_files[spatial_info_index]\n",
    "    which_file = [occursin(experiment_filename_1, candidate_filename[i])*occursin(\"A_dF\", candidate_filename[i]) for i in 1:length(candidate_filename)]\n",
    "    @assert(length(candidate_filename[which_file]) == 1)\n",
    "    save_file_name = candidate_filename[which_file][1]\n",
    "    n_bins = save_file_name[end-4:end-3]\n",
    "    info_filename = joinpath(file_folder_1, save_file_name)\n",
    "    file = h5open(info_filename, \"r\")\n",
    "    place_map_all_1 = HDF5.readmmap(file[\"place_map_all\"])\n",
    "    valid_neuron_1 = HDF5.readmmap(file[\"valid_neurons\"])\n",
    "\n",
    "    close(file)\n",
    "\n",
    "\n",
    "    file_folder_2 = joinpath(data_path(ds_save_cy_2), \"\")\n",
    "    all_files = readdir(file_folder_2)\n",
    "    long_name_files = all_files[findall([length(all_files[i])>6 for i in 1:length(all_files)])]\n",
    "    spatial_info_index = findall([long_name_files[i][1:6]==\"neuron\" for i in 1:length(long_name_files)])\n",
    "    candidate_filename = long_name_files[spatial_info_index]\n",
    "    which_file = [occursin(experiment_filename_2, candidate_filename[i])*occursin(\"A_dF\", candidate_filename[i]) for i in 1:length(candidate_filename)]\n",
    "    println(candidate_filename[which_file])\n",
    "    @assert(length(candidate_filename[which_file]) == 1)\n",
    "    save_file_name = candidate_filename[which_file][1]\n",
    "    info_filename = joinpath(file_folder_2, save_file_name)\n",
    "    file = h5open(info_filename, \"r\")\n",
    "    place_map_all_2 = HDF5.readmmap(file[\"place_map_all\"])\n",
    "    valid_neuron_2 = HDF5.readmmap(file[\"valid_neurons\"])\n",
    "\n",
    "    close(file)\n",
    "\n",
    "\n",
    "    NMF_filename = joinpath(data_path(ds_save_cy_1), \"NMF_merge.h5\")\n",
    "    NMF_file = h5open(NMF_filename, \"r\")\n",
    "    global Z_all = HDF5.readmmap(NMF_file[\"Z_all\"])\n",
    "    global X_all = HDF5.readmmap(NMF_file[\"X_all\"])\n",
    "    global Y_all = HDF5.readmmap(NMF_file[\"Y_all\"])\n",
    "    global A_dFF_1 = HDF5.readmmap(NMF_file[\"A_dF\"]); # bs update\n",
    "    close(NMF_file)\n",
    "\n",
    "    # for multi sessions\n",
    "    NMF_filename = joinpath(data_path(ds_save_cy_2), \"NMF_merge.h5\") # bs update\n",
    "    NMF_file = h5open(NMF_filename, \"r\") # bs update\n",
    "    global A_dFF_2 = HDF5.readmmap(NMF_file[\"A_dF\"]); # bs update\n",
    "    close(NMF_file) # bs update\n",
    "\n",
    "    n_neuron = length(X_all);\n",
    "\n",
    "    save_file_name = \"compare_map_results.h5\"\n",
    "    compare_filename = joinpath(data_path(ds_save_cy_1), save_file_name)\n",
    "    file = h5open(compare_filename, \"r\")\n",
    "    tel_place_cell_index = HDF5.readmmap(file[\"tel_place_cell_index\"])\n",
    "    confined_place_cell_index = HDF5.readmmap(file[\"confined_place_cell_index\"])\n",
    "    confined_place_cell_index_1 = HDF5.readmmap(file[\"confined_place_cell_index_1\"])\n",
    "    confined_place_cell_index_2 = HDF5.readmmap(file[\"confined_place_cell_index_2\"])\n",
    "    peak_loc_map_1 = HDF5.readmmap(file[\"peak_loc_map_1\"])\n",
    "    peak_loc_map_2 = HDF5.readmmap(file[\"peak_loc_map_2\"])\n",
    "    close(file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68b552f-0752-4016-abc8-1dda0c4873dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tel_place_cell_index = intersect(tel_place_cell_index, valid_neuron_1, valid_neuron_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1af01d-c460-49a3-8a8d-8caddd3c1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    file_folder_1 = joinpath(data_path(ds_save_cy_1), \"place_cell_windows\")\n",
    "    all_files = readdir(file_folder_1)\n",
    "    long_name_files = all_files[findall([length(all_files[i])>6 for i in 1:length(all_files)])]\n",
    "    spatial_info_index = findall([long_name_files[i][1:6]==\"neuron\" for i in 1:length(long_name_files)])\n",
    "    candidate_filename = long_name_files[spatial_info_index]\n",
    "    which_file = [occursin(experiment_filename_1, candidate_filename[i])*occursin(\"A_dF\", candidate_filename[i]) for i in 1:length(candidate_filename)]\n",
    "    save_file_name = candidate_filename[which_file][1]\n",
    "    println(save_file_name)   \n",
    "    info_filename = joinpath(data_path(ds_save_cy_1),\"place_cell_windows\", save_file_name)\n",
    "    file = h5open(info_filename, \"r\")\n",
    "    place_map_all_1_early = HDF5.readmmap(file[\"place_map_all\"])\n",
    "    close(file)\n",
    "        \n",
    "    save_file_name = candidate_filename[which_file][2]\n",
    "    println(save_file_name)\n",
    "    info_filename = joinpath(data_path(ds_save_cy_1),\"place_cell_windows\", save_file_name)\n",
    "    file = h5open(info_filename, \"r\")\n",
    "    place_map_all_1_late = HDF5.readmmap(file[\"place_map_all\"])\n",
    "    specificity_1_late = HDF5.readmmap(file[\"specificity\"])\n",
    "    close(file)\n",
    "\n",
    "    which_neuron = tel_place_cell_index\n",
    "\n",
    "    corr_map_pairwise_all_1_early = fill(NaN32, length(which_neuron), length(which_neuron))\n",
    "\n",
    "    for (i, neuron_idx_1) in enumerate(which_neuron)\n",
    "        for (j, neuron_idx_2) in enumerate(which_neuron)\n",
    "            map_1 = place_map_all_1_early[:,:, neuron_idx_1]\n",
    "            map_2 = place_map_all_1_early[:,:, neuron_idx_2]\n",
    "            corr_map_pairwise_all_1_early[i,j] = corr_2d(map_1, map_2)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    corr_map_pairwise_all_1_late = fill(NaN32, length(which_neuron), length(which_neuron))\n",
    "\n",
    "    for (i, neuron_idx_1) in enumerate(which_neuron)\n",
    "        for (j, neuron_idx_2) in enumerate(which_neuron)\n",
    "            map_1 = place_map_all_1_late[:,:, neuron_idx_1]\n",
    "            map_2 = place_map_all_1_late[:,:, neuron_idx_2]\n",
    "            corr_map_pairwise_all_1_late[i,j] = corr_2d(map_1, map_2)\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # histogram of PF correlation\n",
    "    which_neuron = tel_place_cell_index\n",
    "\n",
    "    corr_map_pairwise_all_1 = fill(NaN32, length(which_neuron), length(which_neuron))\n",
    "\n",
    "    for (i, neuron_idx_1) in enumerate(which_neuron)\n",
    "        for (j, neuron_idx_2) in enumerate(which_neuron)\n",
    "            map_1 = place_map_all_1[:,:, neuron_idx_1]\n",
    "            map_2 = place_map_all_1[:,:, neuron_idx_2]\n",
    "            corr_map_pairwise_all_1[i,j] = corr_2d(map_1, map_2)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    corr_map_pairwise_all_2 = fill(NaN32, length(which_neuron), length(which_neuron))\n",
    "\n",
    "    for (i, neuron_idx_1) in enumerate(which_neuron)\n",
    "        for (j, neuron_idx_2) in enumerate(which_neuron)\n",
    "            map_1 = place_map_all_2[:,:, neuron_idx_1]\n",
    "            map_2 = place_map_all_2[:,:, neuron_idx_2]\n",
    "            corr_map_pairwise_all_2[i,j] = corr_2d(map_1, map_2)\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "\n",
    "    which_neuron = tel_place_cell_index\n",
    "    cell_locs = reduce(hcat, [X_all, Y_all, Z_all.*2])[which_neuron,:];\n",
    "    anatomy_pairwise_distance =  metrics.pairwise_distances(cell_locs);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99bab8-ee5c-4263-9c2a-1fe8ed593af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pyimport sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48d08b-9485-42c3-9154-7ea84ea05883",
   "metadata": {},
   "outputs": [],
   "source": [
    "@showprogress for which_data = 1:length(data_info_all)\n",
    "    data_info = data_info_all[which_data]\n",
    "\n",
    "    experiment_filename_1 = data_info[1]\n",
    "    server_1 = data_info[2]\n",
    "\n",
    "    experiment_filename_2 = data_info[3]\n",
    "    server_2 = data_info[4]\n",
    "\n",
    "    experimenter = data_info[5]\n",
    "\n",
    "    ds_save_1 = Dataset(experiment_filename_1, experimenter, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_cy_1 = Dataset(experiment_filename_1, \"chuyu\", gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_analyzer_1 = Dataset(experiment_filename_1, analyzer, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "\n",
    "    ds_save_2 = Dataset(experiment_filename_2, experimenter, gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\") # This example dataset is on roli-9, so the path is different depending on whether you're trying to access the file from roli-9\n",
    "    ds_save_cy_2 = Dataset(experiment_filename_2, \"chuyu\", gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\")\n",
    "    ds_save_analyzer_2 = Dataset(experiment_filename_2, analyzer, gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\")\n",
    "\n",
    "\n",
    "    file_folder_1 = joinpath(data_path(ds_save_cy_1), \"\")\n",
    "    all_files = readdir(file_folder_1)\n",
    "    long_name_files = all_files[findall([length(all_files[i])>6 for i in 1:length(all_files)])]\n",
    "    spatial_info_index = findall([long_name_files[i][1:6]==\"neuron\" for i in 1:length(long_name_files)])\n",
    "    candidate_filename = long_name_files[spatial_info_index]\n",
    "    which_file = [occursin(experiment_filename_1, candidate_filename[i])*occursin(\"A_dF\", candidate_filename[i]) for i in 1:length(candidate_filename)]\n",
    "    @assert(length(candidate_filename[which_file]) == 1)\n",
    "    save_file_name = candidate_filename[which_file][1]\n",
    "    n_bins = save_file_name[end-4:end-3]\n",
    "    info_filename = joinpath(file_folder_1, save_file_name)\n",
    "    file = h5open(info_filename, \"r\")\n",
    "    place_map_all_1 = HDF5.readmmap(file[\"place_map_all\"])\n",
    "    close(file)\n",
    "\n",
    "\n",
    "    file_folder_2 = joinpath(data_path(ds_save_cy_2), \"\")\n",
    "    all_files = readdir(file_folder_2)\n",
    "    long_name_files = all_files[findall([length(all_files[i])>6 for i in 1:length(all_files)])]\n",
    "    spatial_info_index = findall([long_name_files[i][1:6]==\"neuron\" for i in 1:length(long_name_files)])\n",
    "    candidate_filename = long_name_files[spatial_info_index]\n",
    "    which_file = [occursin(experiment_filename_2, candidate_filename[i])*occursin(\"A_dF\", candidate_filename[i]) for i in 1:length(candidate_filename)]\n",
    "    println(candidate_filename[which_file])\n",
    "    @assert(length(candidate_filename[which_file]) == 1)\n",
    "    save_file_name = candidate_filename[which_file][1]\n",
    "    info_filename = joinpath(file_folder_2, save_file_name)\n",
    "    file = h5open(info_filename, \"r\")\n",
    "    place_map_all_2 = HDF5.readmmap(file[\"place_map_all\"])\n",
    "    close(file)\n",
    "\n",
    "\n",
    "    NMF_filename = joinpath(data_path(ds_save_cy_1), \"NMF_merge.h5\")\n",
    "    NMF_file = h5open(NMF_filename, \"r\")\n",
    "    global Z_all = HDF5.readmmap(NMF_file[\"Z_all\"])\n",
    "    global X_all = HDF5.readmmap(NMF_file[\"X_all\"])\n",
    "    global Y_all = HDF5.readmmap(NMF_file[\"Y_all\"])\n",
    "    global A_dFF_1 = HDF5.readmmap(NMF_file[\"A_dF\"]); # bs update\n",
    "    close(NMF_file)\n",
    "\n",
    "    # for multi sessions\n",
    "    NMF_filename = joinpath(data_path(ds_save_cy_2), \"NMF_merge.h5\") # bs update\n",
    "    NMF_file = h5open(NMF_filename, \"r\") # bs update\n",
    "    global A_dFF_2 = HDF5.readmmap(NMF_file[\"A_dF\"]); # bs update\n",
    "    close(NMF_file) # bs update\n",
    "\n",
    "    n_neuron = length(X_all);\n",
    "\n",
    "    save_file_name = \"compare_map_results.h5\"\n",
    "    compare_filename = joinpath(data_path(ds_save_cy_1), save_file_name)\n",
    "    file = h5open(compare_filename, \"r\")\n",
    "    tel_place_cell_index = HDF5.readmmap(file[\"tel_place_cell_index\"])\n",
    "    confined_place_cell_index = HDF5.readmmap(file[\"confined_place_cell_index\"])\n",
    "    confined_place_cell_index_1 = HDF5.readmmap(file[\"confined_place_cell_index_1\"])\n",
    "    confined_place_cell_index_2 = HDF5.readmmap(file[\"confined_place_cell_index_2\"])\n",
    "    peak_loc_map_1 = HDF5.readmmap(file[\"peak_loc_map_1\"])\n",
    "    peak_loc_map_2 = HDF5.readmmap(file[\"peak_loc_map_2\"])\n",
    "    close(file)\n",
    "\n",
    "    file_folder_1 = joinpath(data_path(ds_save_cy_1), \"place_cell_windows\")\n",
    "    all_files = readdir(file_folder_1)\n",
    "    long_name_files = all_files[findall([length(all_files[i])>6 for i in 1:length(all_files)])]\n",
    "    spatial_info_index = findall([long_name_files[i][1:6]==\"neuron\" for i in 1:length(long_name_files)])\n",
    "    candidate_filename = long_name_files[spatial_info_index]\n",
    "    which_file = [occursin(experiment_filename_1, candidate_filename[i])*occursin(\"A_dF\", candidate_filename[i]) for i in 1:length(candidate_filename)]\n",
    "    save_file_name = candidate_filename[which_file][1]\n",
    "    println(save_file_name)   \n",
    "    info_filename = joinpath(data_path(ds_save_cy_1),\"place_cell_windows\", save_file_name)\n",
    "    file = h5open(info_filename, \"r\")\n",
    "    place_map_all_1_early = HDF5.readmmap(file[\"place_map_all\"])\n",
    "    close(file)\n",
    "        \n",
    "    save_file_name = candidate_filename[which_file][2]\n",
    "    println(save_file_name)\n",
    "    info_filename = joinpath(data_path(ds_save_cy_1),\"place_cell_windows\", save_file_name)\n",
    "    file = h5open(info_filename, \"r\")\n",
    "    place_map_all_1_late = HDF5.readmmap(file[\"place_map_all\"])\n",
    "    specificity_1_late = HDF5.readmmap(file[\"specificity\"])\n",
    "    close(file)\n",
    "\n",
    "    which_neuron = tel_place_cell_index\n",
    "\n",
    "    corr_map_pairwise_all_1_early = fill(NaN32, length(which_neuron), length(which_neuron))\n",
    "\n",
    "    for (i, neuron_idx_1) in enumerate(which_neuron)\n",
    "        for (j, neuron_idx_2) in enumerate(which_neuron)\n",
    "            map_1 = place_map_all_1_early[:,:, neuron_idx_1]\n",
    "            map_2 = place_map_all_1_early[:,:, neuron_idx_2]\n",
    "            corr_map_pairwise_all_1_early[i,j] = corr_2d(map_1, map_2)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    corr_map_pairwise_all_1_late = fill(NaN32, length(which_neuron), length(which_neuron))\n",
    "\n",
    "    for (i, neuron_idx_1) in enumerate(which_neuron)\n",
    "        for (j, neuron_idx_2) in enumerate(which_neuron)\n",
    "            map_1 = place_map_all_1_late[:,:, neuron_idx_1]\n",
    "            map_2 = place_map_all_1_late[:,:, neuron_idx_2]\n",
    "            corr_map_pairwise_all_1_late[i,j] = corr_2d(map_1, map_2)\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "\n",
    "    pairwise_distance_peak_1 = metrics.pairwise_distances(peak_loc_map_1[:,whether_in(confined_place_cell_index_1, confined_place_cell_index_2)]')\n",
    "    pairwise_distance_peak_2 = metrics.pairwise_distances(peak_loc_map_2[:,whether_in(confined_place_cell_index_2, confined_place_cell_index_1)]');\n",
    "\n",
    "\n",
    "    # histogram of PF correlation\n",
    "    which_neuron = tel_place_cell_index\n",
    "\n",
    "    corr_map_pairwise_all_1 = fill(NaN32, length(which_neuron), length(which_neuron))\n",
    "\n",
    "    for (i, neuron_idx_1) in enumerate(which_neuron)\n",
    "        for (j, neuron_idx_2) in enumerate(which_neuron)\n",
    "            map_1 = place_map_all_1[:,:, neuron_idx_1]\n",
    "            map_2 = place_map_all_1[:,:, neuron_idx_2]\n",
    "            corr_map_pairwise_all_1[i,j] = corr_2d(map_1, map_2)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    corr_map_pairwise_all_2 = fill(NaN32, length(which_neuron), length(which_neuron))\n",
    "\n",
    "    for (i, neuron_idx_1) in enumerate(which_neuron)\n",
    "        for (j, neuron_idx_2) in enumerate(which_neuron)\n",
    "            map_1 = place_map_all_2[:,:, neuron_idx_1]\n",
    "            map_2 = place_map_all_2[:,:, neuron_idx_2]\n",
    "            corr_map_pairwise_all_2[i,j] = corr_2d(map_1, map_2)\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # histogram of PF correlation\n",
    "    which_neuron = tel_place_cell_index\n",
    "\n",
    "    corr_activity_pairwise_all_1 = fill(NaN32, length(which_neuron), length(which_neuron))\n",
    "\n",
    "    for (i, neuron_idx_1) in enumerate(which_neuron)\n",
    "        for (j, neuron_idx_2) in enumerate(which_neuron)\n",
    "            activity_1 = A_dFF_1[:, neuron_idx_1]\n",
    "            activity_2 = A_dFF_1[:, neuron_idx_2]\n",
    "            corr_activity_pairwise_all_1[i,j] = corr_nan(activity_1, activity_2)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    corr_activity_pairwise_all_2 = fill(NaN32, length(which_neuron), length(which_neuron))\n",
    "\n",
    "    for (i, neuron_idx_1) in enumerate(which_neuron)\n",
    "        for (j, neuron_idx_2) in enumerate(which_neuron)\n",
    "            activity_1 = A_dFF_2[:, neuron_idx_1]\n",
    "            activity_2 = A_dFF_2[:, neuron_idx_2]\n",
    "            corr_activity_pairwise_all_2[i,j] = corr_nan(activity_1, activity_2)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    which_neuron = tel_place_cell_index\n",
    "    cell_locs = reduce(hcat, [X_all, Y_all, Z_all.*2])[which_neuron,:];\n",
    "    anatomy_pairwise_distance =  metrics.pairwise_distances(cell_locs);\n",
    "\n",
    "\n",
    "    h5open(joinpath(data_path(ds_save_analyzer_1), \"cluster_analysis.h5\"), \"w\") do file\n",
    "        file[\"corr_map_pairwise_all_1_early\"] = corr_map_pairwise_all_1_early\n",
    "        file[\"corr_map_pairwise_all_1_late\"] = corr_map_pairwise_all_1_late\n",
    "        file[\"pairwise_distance_peak_1\"] = pairwise_distance_peak_1\n",
    "        file[\"pairwise_distance_peak_2\"] = pairwise_distance_peak_2\n",
    "        file[\"corr_map_pairwise_all_1\"] = corr_map_pairwise_all_1\n",
    "        file[\"corr_map_pairwise_all_2\"] = corr_map_pairwise_all_2\n",
    "        file[\"corr_activity_pairwise_all_1\"] = corr_activity_pairwise_all_1;\n",
    "        file[\"corr_activity_pairwise_all_2\"] = corr_activity_pairwise_all_2;\n",
    "        file[\"anatomy_pairwise_distance\"] = anatomy_pairwise_distance;   \n",
    "\n",
    "    end;\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1791e8-0dd8-4f40-b7a0-0fd554dd1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for which_data in 1:length(data_info_all)\n",
    "    try\n",
    "    data_info = data_info_all[which_data]\n",
    "\n",
    "    experiment_filename_1 = data_info[1]\n",
    "    server_1 = data_info[2]\n",
    "\n",
    "    experiment_filename_2 = data_info[3]\n",
    "    server_2 = data_info[4]\n",
    "\n",
    "    experimenter = data_info[5]\n",
    "\n",
    "    ds_save_1 = Dataset(experiment_filename_1, experimenter, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_cy_1 = Dataset(experiment_filename_1, \"chuyu\", gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "    ds_save_analyzer_1 = Dataset(experiment_filename_1, analyzer, gethostname() == \"roli-$(server_1)\" ? \"/data\" : \"/nfs/data$(server_1)\")\n",
    "\n",
    "    ds_save_2 = Dataset(experiment_filename_2, experimenter, gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\") # This example dataset is on roli-9, so the path is different depending on whether you're trying to access the file from roli-9\n",
    "    ds_save_cy_2 = Dataset(experiment_filename_2, \"chuyu\", gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\")\n",
    "    ds_save_analyzer_2 = Dataset(experiment_filename_2, analyzer, gethostname() == \"roli-$(server_2)\" ? \"/data\" : \"/nfs/data$(server_2)\")\n",
    "\n",
    "\n",
    "    all_files = readdir(data_path(ds_save_cy_1))\n",
    "    long_name_files = all_files[findall([length(all_files[i])>6 for i in 1:length(all_files)])]\n",
    "    spatial_info_index = findall([long_name_files[i][1:6]==\"compar\" for i in 1:length(long_name_files)])\n",
    "    # println(long_name_files[spatial_info_index])\n",
    "    \n",
    "    \n",
    "    # println(which_data)\n",
    "    \n",
    "    path_target = data_path(ds_save_cy_1)\n",
    "    run(`ls -lha $path_target/cluster_analysis.h5`)\n",
    "\n",
    "    catch e\n",
    "        println(e)\n",
    "    end\n",
    "    \n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ec579-64bf-48e0-9def-e01026f4a140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.4",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
